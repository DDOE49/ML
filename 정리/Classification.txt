# Classification

- 분류를 뜻하는 지도학습의 일종으로 기존에 존재하는 데이터의 Category 관계를 파악하고 새롭게 관측된 데이터의 Category를 스스로 판별하는 과정
ex) 문자를 판별하여, 스팸 보관함으로 분류하는 것과 같은 단일분류,
	수능 점수가 몇 등급에 해당하는지 판별하는 종류의 다즁분류(비지도학습의 Clustering과 비슷하지만 가장 큰 차이점은 Category의 도메인이 정의되있는가 그렇지 않은가이다)

지도학습의 Classification은 이미 정해진 카테고리(레이블) 안에서 학습하여 새로운 데이터를 분류하지만,

비지도학습의 Clustering은 정해지지 않은 카테고리(레이블)를 원하는 만큼 생성하여, 분류하는것이 가장 큰 차이점이다. 

* 종류
- KNN:
 데이터를 분류하고 새로운 데이터 포인트의 카테고리를 결정할 때 K 개의 가장 가까운 포인트를 선점하고 그중 가장 많이 선택된 포인트의 카테고리로 이 새로운 데이터를 분류하는 방법
 k-nearest neighbor에서 고려해야 할 사항은 알고리즘의 핵심 부분이 대상 포인트와의 거리에 대한 측정이고, 이를 계산하는 방법으로 무조건 유클리드 거리 측정 방식을 사용하는 것을 자제해야 한다. 모든 데이터 열을 이처럼 같은 방식으로 처리하면 생각하지 못한 변수에 의해 오류가 생길 수 있으므로 거리의 제곱을 합산하기 전 각 카테고리에 대한 평균 거리를 빼고 계산하는 방식과 같은 다양한 거리 계산 알고리즘에 대한 논의가 필요하다. 예를 들어 실수 데이터의 경우 유클리드 거리 측정 방식을 사용하고, 범주형 혹은 이진 데이터와 같은 유형의 데이터는 해밍 거리 측정 방식을 사용한다.

- Decision Tree(의사결정 트리):
 가장 단순한 classifier 중 하나로, decision tree와 같은 도구를 활용하여 모델을 그래프로 그리는 매우 단순한 구조로 되어 있다. 이 방식은 root에서부터 적절한 node를 선택하면서 진행하다가 최종 결정을 내리게 되는 model이다.
 이 트리의 장점은 누구나 쉽게 이해할 수 있고, 결과를 해석할수있다.
 예를 들어 yes를 선택했던 것을 no로 바꾸기만 하면 간단하게 로직을 바꿀 수 있다.
 가지고 있는 데이터의 Feature를 분석해서 Tree를  Build하는 과정이 제일 중요하다.

- Random Forest:
Decision tree가 여러개 모여 Forest를 이룬 것이다.
Decision tree보다 작은 Tree가 여러개 모이게 되어, 모든 트리의 결과들을 합하여 더많은 값을 최종결과로 본다.

- Naive Bayes(나이브 베이즈):
확률을 사용한다.
Naive Bayes의 이론에 따라 계산

- SVM(Support Vector Machine):
Decision Boundary라는 직선이 주어진 상태


* 모델링과 Classifier가 있는 상태에서, Testing을 하기전에 반드시 해야할 것이 validation

- K-Fold Cross Validation:
고전적인 방법으로는 train data에서 일부를 떼서 validation에 쓰는 것이다.그 결과, train을 통해 나오는 accuracy는 validation dataset의 accuracy이다. 여기서 문제점은, train에서 떼어낸 특정 부분만의 accuracy이며, 다른부분에서 떼어낼 경우 accuracy가 달라지게 된다. 즉, bias(편견이 있는) validation이다.

k-fold cross validation방법은 고전적인 valdation방법을 보완한다.

1) 먼저 k=10으로 가정하면, train data를 10등분한다.
2) 순서대로 돌아가면서 1/10을 validation으로 사용하며, 나머지 9개를 train으로 사용한다.
3) 10번의 validation결과로 나온 accuracy를 평균낸다. 전문용어로는 validation accuracy score라고 한다.


