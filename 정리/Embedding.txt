# Embedding

1. 자연어 처리에서 임베딩은 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자형태인 벡터로 바꾼 결과 혹은 그 일련의 과정 전체를 의미

2. 임베딩 역할

 - 단어/문장 간 관련도 계산(word2vec) :
	 단어를 전체 단어들간의 관계에 맞춰 해당 단어의 특성을 갖는 벡터로 바꾸면 단어들 사이의 유사도를 계산하는 일이 가능
	 word2vec은 유사한 의미를 가진 단어는 유사한 벡터가 되는 특징이 있다.

 - 의미적/문법적 정보 함축 :
	 단어 벡터 간 덧셈/뺄셈을 통해 단어들 사이의 의미적, 문법적 관계를 도출해낼 수 있다
	 아들 - 딸 사이의 관계와 소년 - 소녀 사이의 의미 차이가 임베딩에 함축돼 있으면 품질이 좋은 임베딩이라 말할 수 있다는 이야기
	 단어 유추 평가(word analogy test)라고 부른다.

 - 전이학습 :
	 품질 좋은 임베딩은 모형의 성능과 모형의 수렴속도가 빨라지는데 이런 품질 좋은 임베딩을 다른 딥러닝 모델의 입력값으로 사용하는 것을 transfer learning이라 한다.

3. 임베딩 기법 종류

 - 통계 기반 기법 :
	 잠재 의미 분석 : 단어 사용 빈도 등 Corpus의 통계량 정보가 들어 있는 행렬에 특이값 분해등 수학적 기법을 적용해 행렬에 속한 벡터들의 차원을 축소하는 방법(메모리 낭비 예방)
	 잠재 의미 분석 수행 대상 행렬은 여러 종류가 될 수 있으며, Term-Document Matrix, TF-IDF Matrix, Word-Context Matrix, PMI Matrix등이 있다.

 - neural Network 기반 기법 :
 	 구조가 유연하고 표현력이 풍부하기 때문에 자연어의 무한한 문맥을 상당 부분 학습할 수 있다.

 - 단어 수준 임베딩 기법 :
 	 각각의 벡터에 해당 단어의 문맥적 의미를 함축하지만, 단어의 형태가 동일하다면 동일단어로 인식하고, 모든 문맥 정보를 해당 단어 벡터 투영하므로 동음이의어를 분간하기 어렵다는 단점이 있다.
 	 ex) NPLM, Word2Vec, GloVe, FastText, Swivel 등

 - 문장 수준 임베딩 기법 :
 	 2018년 초에 ELMo(Embedding from Language Models)가 발표된 이후 주목 받기 시작했다. 개별 단어가 아닌 단어 Sequence 전체의 문맥적 의미를 함축 하기 때문에 단어 임베딩 기법보다 Transfer learning 효과가 좋은 것으로 알려져 있다. 또한, 단어 수준 임베딩의 단점인 동음이의어도 문장수준 임베딩 기법을 사용하면 분리해서 이해할 수 있다.
 	 ex) BERT, GPT

 - Rule based -> End to End -> Pre-training/fine tuning


4. 임베딩의 종류와 성능

 - 행렬 분해 :
 	 Corpus 정보가 들어 있는 원래 행렬을 Decomposition을 통해 임베딩하는 기법이다. Decomposition 이후엔 둘 중 하나의 행렬만 사용하거나 둘을 sum하거나 concatenate하는 방식으로 임베딩을 한다.
 	 ex) GloVe, Swivel 등

 - 예측 기반 :
 	 어떤 단어 주변에 특정 단어가 나타날지 예측하거나, 이전 단어들이 주어졌을 때 다음 단어가 무엇일지 예측하거나, 문장 내 일부 단어를 지우고 해당 단어가 무엇일지 맞추는 과정에서 학습하는 방법
 	 ex) Neural Network기반 방법들이 속한다. ex) Word2Vec, FastText, BERT, ELMo, GPT 등

 - 토픽 기반 :
 	 주어진 문서에 잠재된 주제를 추론하는 방식으로 임베딩을 수행하는 기법이며, 대표적으로 잠재 디리클레 할당(LDA)가 있다. LDA 같은 모델은 학습이 완료되면 각 문서가 어떤 주제 분포를 갖는지 확률 벡터 형태로 반환하기 때문에 임베딩 기법의 일종으로 이해할 수 있다.


https://heung-bae-lee.github.io/2020/01/16/NLP_01/